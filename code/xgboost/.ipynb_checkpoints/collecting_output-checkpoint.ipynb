{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: iter 1\n",
      "INFO: iter 10001\n",
      "INFO: iter 20001\n",
      "INFO: iter 30001\n",
      "INFO: iter 40001\n",
      "INFO: iter 50001\n",
      "INFO: iter 60001\n",
      "INFO: iter 70001\n",
      "INFO: iter 80001\n",
      "INFO: iter 90001\n",
      "INFO: iter 100001\n",
      "----- INFO: training data preprocess is done. -----\n"
     ]
    }
   ],
   "source": [
    "#!/bin/python\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "import sklearn.model_selection\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle\n",
    "import xgboost as xgb\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, train_test_split, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, mean_squared_error, precision_score, accuracy_score\n",
    "\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# argument parsing \n",
    "#parser = argparse.ArgumentParser(description='Process some integers.')\n",
    "#parser.add_argument('integers', metavar='N', type=int, nargs='+',\n",
    "#                   help='an integer for the accumulator')\n",
    "#parser.add_argument('--sum', dest='accumulate', action='store_const',\n",
    "#                   const=sum, default=max,\n",
    "#                   help='sum the integers (default: find the max)')\n",
    "\n",
    "#args = parser.parse_args()\n",
    "#idx_lr = sys.argv[1]\n",
    "idx_lr = 1\n",
    "#idx_md = sys.argv[2]\n",
    "idx_md = 1\n",
    "\n",
    "# set seed \n",
    "rng = np.random.RandomState(1)\n",
    "\n",
    "# set directory\n",
    "os.getcwd()\n",
    "os.chdir(\"/ysm-gpfs/pi/zhao/zy92/projects/ddipred/ddi_pred/data\")\n",
    "\n",
    "# list of desired DDI Types\n",
    "desired_DDI = [0, 1, 2, 3, 4, 5, 6, 7, 15, 16, 17, 18, 19, 20, 21, 22, 26, 28, 30, 31, 32, 38, 40, 41, 43, 44, 45,\n",
    "               49, 50, 51, 52, 54, 55, 62, 67, 68, 72, 74, 76, 78, 79, 80, 81]\n",
    "\n",
    "\n",
    "# parameters\n",
    "parameters = {\"learning_rate\"    : [0.01, 0.10, 0.25, 0.5, 1 ],\n",
    "    \"max_depth\"        : [ 3, 4, 5, 6, 7],\n",
    "    \"min_child_weight\" : [ 1, 3, 5, 7 ],\n",
    "    \"gamma\"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],\n",
    "    \"colsample_bytree\" : [ 0.3, 0.4, 0.5 , 0.7 ] }\n",
    "\n",
    "# load the data \n",
    "ddidata = pd.read_excel(\"DrugBank_known_ddi.xlsx\")\n",
    "interactiondict = pd.read_csv(\"Interaction_information.csv\")\n",
    "safe_drugs = pd.read_csv(\"safe_drug_combos.csv\")\n",
    "drug_similarity_feature = pd.read_csv(\"drug_similarity.csv\")\n",
    "drug_similarity = drug_similarity_feature.iloc[:, 1:len(drug_similarity_feature)+1]\n",
    "\n",
    "# filter ddidata for desired DDI types\n",
    "up_ddidata = ddidata[ddidata.Label.isin(desired_DDI)]\n",
    "new_ddidata = up_ddidata.copy()\n",
    "\n",
    "# convert types to int\n",
    "new_ddidata.drug1 = up_ddidata.drug1.str[2:].astype(int)\n",
    "new_ddidata.drug2 = up_ddidata.drug2.str[2:].astype(int)\n",
    "new_ddidata.Label = up_ddidata.Label\n",
    "\n",
    "\n",
    "# incorporate safe_drugs into new_ddidata with DDIType 0\n",
    "safe_drugs[\"Label\"] = 0\n",
    "\n",
    "frames = [safe_drugs, new_ddidata]\n",
    "ddi_df = pd.concat(frames)\n",
    "\n",
    "# create a DB to index dictionary from similarity dataset\n",
    "DB_to_index = {}\n",
    "i = 0\n",
    "for col in drug_similarity.columns:\n",
    "    DB_to_index[int(col[2:7])] = i\n",
    "    i = i + 1\n",
    "\n",
    "# filter output to only include DBs with similarity features\n",
    "ddi_df_output = ddi_df[ddi_df.drug1.isin(DB_to_index)]\n",
    "ddi_output = ddi_df_output[ddi_df_output.drug2.isin(DB_to_index)]\n",
    "\n",
    "# filter out the duplicate samples\n",
    "bool_series_to_delete = ddi_output[['drug1', 'drug2']].duplicated()\n",
    "ddi_clean = ddi_output[~bool_series_to_delete]\n",
    "\n",
    "\n",
    "# feature building \n",
    "# add similarity feature for each drug-drug pair\n",
    "n_similarity = 2159\n",
    "sim_array = np.empty([ddi_clean.shape[0], 2*n_similarity], dtype='float16')\n",
    "i = 0\n",
    "for index, (_, row) in enumerate(ddi_clean.iterrows()):\n",
    "    if index % 10000 == 0:\n",
    "        print(\"INFO: iter \" + str(index + 1))\n",
    "    drug1_index = DB_to_index[row[\"drug1\"]]\n",
    "    drug2_index = DB_to_index[row[\"drug2\"]]\n",
    "    feature_vec = np.hstack([drug_similarity.iloc[:,drug1_index],drug_similarity.iloc[:,drug2_index]])\n",
    "    sim_array[index, ] = feature_vec\n",
    "    \n",
    "# create input and output vectors for training\n",
    "\n",
    "X_data = sim_array\n",
    "y_data = np.array(ddi_clean.Label)\n",
    "\n",
    "# transform the y\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_data)\n",
    "encoded_Y = encoder.transform(y_data)\n",
    "\n",
    "#y_data = tf.keras.utils.to_categorical(encoded_Y)\n",
    "y_data = encoded_Y\n",
    "\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X_data, y_data, test_size = 0.3\n",
    "                                                                           , random_state = 1)\n",
    "X_test, X_val, y_test, y_val = sklearn.model_selection.train_test_split(X_test, y_test, test_size = 0.5\n",
    "                                                                       , random_state = 1)\n",
    "\n",
    "# dmatrix\n",
    "dtrain = xgb.DMatrix(X_train, label = y_train)\n",
    "dtest = xgb.DMatrix(X_test, label = y_test)\n",
    "dval = xgb.DMatrix(X_val, label = y_val)\n",
    "\n",
    "print(\"----- INFO: training data preprocess is done. -----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: processing number1\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'learning_rate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-9ebba555ff62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"learning_rate\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_lr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mhistory_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_lr_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_md_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_history_dict.pickle\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_lr_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_md_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_xgb.pickle\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mhistory_fo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'learning_rate' is not defined"
     ]
    }
   ],
   "source": [
    "# param\n",
    "output_dir = \"/ysm-gpfs/pi/zhao/zy92/projects/ddipred/ddi_pred/code/xgboost/output/\"\n",
    "model_dict = {}\n",
    "count = 0\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        print(\"INFO: \" + \"processing number\" + str(count + 1))\n",
    "        learning_rate = parameters[\"learning_rate\"][idx_lr]\n",
    "        max_depth = parameters[\"max_depth\"][i]\n",
    "        count += 1\n",
    "        history_path = output_dir + \"_lr_\" + str(learning_rate) + \"_md_\" + str(max_depth) + \"_history_dict.pickle\"\n",
    "        model_path = output_dir + \"_lr_\" + str(learning_rate) + \"_md_\" + str(max_depth) + \"_xgb.pickle\"\n",
    "        history_fo = open(history_path, \"rb\")\n",
    "        model_fo = open(model_path, \"rb\")\n",
    "        history_dict = pickle.load(history_fo)\n",
    "        bst = pickle.load(model_fo)\n",
    "        # load the object from the file into var b\n",
    "        model_dict[count] = {'model': bst,\n",
    "                             'history': history_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
