{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: iter 1\n",
      "INFO: iter 10001\n",
      "INFO: iter 20001\n",
      "INFO: iter 30001\n",
      "INFO: iter 40001\n",
      "INFO: iter 50001\n",
      "INFO: iter 60001\n",
      "INFO: iter 70001\n",
      "INFO: iter 80001\n",
      "INFO: iter 90001\n",
      "INFO: iter 100001\n",
      "----- INFO: training data preprocess is done. -----\n",
      "----- INFO: XGBoost Parameters -----\n",
      "{'max_depth': 4, 'learning_rate': 0.1, 'eta': 0.3, 'verbosity': 3, 'objective': 'multi:softprob', 'eval_metric': ['merror', 'mlogloss'], 'num_class': 43}\n"
     ]
    }
   ],
   "source": [
    "#!/bin/python\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "import sklearn.model_selection\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle\n",
    "import xgboost as xgb\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, train_test_split, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, mean_squared_error, precision_score, accuracy_score\n",
    "\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# argument parsing \n",
    "#parser = argparse.ArgumentParser(description='Process some integers.')\n",
    "#parser.add_argument('integers', metavar='N', type=int, nargs='+',\n",
    "#                   help='an integer for the accumulator')\n",
    "#parser.add_argument('--sum', dest='accumulate', action='store_const',\n",
    "#                   const=sum, default=max,\n",
    "#                   help='sum the integers (default: find the max)')\n",
    "\n",
    "#args = parser.parse_args()\n",
    "#idx_lr = sys.argv[1]\n",
    "idx_lr = 1\n",
    "#idx_md = sys.argv[2]\n",
    "idx_md = 1\n",
    "\n",
    "# set seed \n",
    "rng = np.random.RandomState(1)\n",
    "\n",
    "# set directory\n",
    "os.getcwd()\n",
    "os.chdir(\"/ysm-gpfs/pi/zhao/zy92/projects/ddipred/ddi_pred/data\")\n",
    "\n",
    "# list of desired DDI Types\n",
    "desired_DDI = [0, 1, 2, 3, 4, 5, 6, 7, 15, 16, 17, 18, 19, 20, 21, 22, 26, 28, 30, 31, 32, 38, 40, 41, 43, 44, 45,\n",
    "               49, 50, 51, 52, 54, 55, 62, 67, 68, 72, 74, 76, 78, 79, 80, 81]\n",
    "\n",
    "\n",
    "# parameters\n",
    "parameters = {\"learning_rate\"    : [0.01, 0.10, 0.25, 0.5, 1 ],\n",
    "    \"max_depth\"        : [ 3, 4, 5, 6, 7],\n",
    "    \"min_child_weight\" : [ 1, 3, 5, 7 ],\n",
    "    \"gamma\"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],\n",
    "    \"colsample_bytree\" : [ 0.3, 0.4, 0.5 , 0.7 ] }\n",
    "\n",
    "# load the data \n",
    "ddidata = pd.read_excel(\"DrugBank_known_ddi.xlsx\")\n",
    "interactiondict = pd.read_csv(\"Interaction_information.csv\")\n",
    "safe_drugs = pd.read_csv(\"safe_drug_combos.csv\")\n",
    "drug_similarity_feature = pd.read_csv(\"drug_similarity.csv\")\n",
    "drug_similarity = drug_similarity_feature.iloc[:, 1:len(drug_similarity_feature)+1]\n",
    "\n",
    "# filter ddidata for desired DDI types\n",
    "up_ddidata = ddidata[ddidata.Label.isin(desired_DDI)]\n",
    "new_ddidata = up_ddidata.copy()\n",
    "\n",
    "# convert types to int\n",
    "new_ddidata.drug1 = up_ddidata.drug1.str[2:].astype(int)\n",
    "new_ddidata.drug2 = up_ddidata.drug2.str[2:].astype(int)\n",
    "new_ddidata.Label = up_ddidata.Label\n",
    "\n",
    "\n",
    "# incorporate safe_drugs into new_ddidata with DDIType 0\n",
    "safe_drugs[\"Label\"] = 0\n",
    "\n",
    "frames = [safe_drugs, new_ddidata]\n",
    "ddi_df = pd.concat(frames)\n",
    "\n",
    "# create a DB to index dictionary from similarity dataset\n",
    "DB_to_index = {}\n",
    "i = 0\n",
    "for col in drug_similarity.columns:\n",
    "    DB_to_index[int(col[2:7])] = i\n",
    "    i = i + 1\n",
    "\n",
    "# filter output to only include DBs with similarity features\n",
    "ddi_df_output = ddi_df[ddi_df.drug1.isin(DB_to_index)]\n",
    "ddi_output = ddi_df_output[ddi_df_output.drug2.isin(DB_to_index)]\n",
    "\n",
    "# filter out the duplicate samples\n",
    "bool_series_to_delete = ddi_output[['drug1', 'drug2']].duplicated()\n",
    "ddi_clean = ddi_output[~bool_series_to_delete]\n",
    "\n",
    "\n",
    "# feature building \n",
    "# add similarity feature for each drug-drug pair\n",
    "n_similarity = 2159\n",
    "sim_array = np.empty([ddi_clean.shape[0], 2*n_similarity], dtype='float16')\n",
    "i = 0\n",
    "for index, (_, row) in enumerate(ddi_clean.iterrows()):\n",
    "    if index % 10000 == 0:\n",
    "        print(\"INFO: iter \" + str(index + 1))\n",
    "    drug1_index = DB_to_index[row[\"drug1\"]]\n",
    "    drug2_index = DB_to_index[row[\"drug2\"]]\n",
    "    feature_vec = np.hstack([drug_similarity.iloc[:,drug1_index],drug_similarity.iloc[:,drug2_index]])\n",
    "    sim_array[index, ] = feature_vec\n",
    "    \n",
    "# create input and output vectors for training\n",
    "\n",
    "X_data = sim_array\n",
    "y_data = np.array(ddi_clean.Label)\n",
    "\n",
    "# transform the y\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_data)\n",
    "encoded_Y = encoder.transform(y_data)\n",
    "\n",
    "#y_data = tf.keras.utils.to_categorical(encoded_Y)\n",
    "y_data = encoded_Y\n",
    "\n",
    "#X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X_data, y_data, test_size = 0.3\n",
    "#                                                                           , random_state = 1)\n",
    "#X_test, X_val, y_test, y_val = sklearn.model_selection.train_test_split(X_test, y_test, test_size = 0.5\n",
    "#                                                                       , random_state = 1)\n",
    "\n",
    "# dmatrix\n",
    "dtrain = xgb.DMatrix(X_train, label = y_train)\n",
    "dtest = xgb.DMatrix(X_test, label = y_test)\n",
    "dval = xgb.DMatrix(X_val, label = y_val)\n",
    "\n",
    "print(\"----- INFO: training data preprocess is done. -----\")\n",
    "# param\n",
    "max_depth = parameters[\"max_depth\"][idx_md]\n",
    "learning_rate = parameters[\"learning_rate\"][idx_lr]\n",
    "\n",
    "\n",
    "param = {\n",
    "    'max_depth': max_depth,  # the maximum depth of each tree\n",
    "    'learning_rate': learning_rate,\n",
    "    'eta': 0.3,  # the training step for each iteration\n",
    "    #'silent': 3,  # logging mode - quiet\n",
    "    'verbosity': 3,  # debug mode\n",
    "    'objective': 'multi:softprob',  # error evaluation for multiclass training\n",
    "    'eval_metric': ['merror', 'mlogloss'],\n",
    "    'num_class': 43}  # the number of classes that exist in this datset\n",
    "num_round = 1  # the number of training iterations\n",
    "\n",
    "print(\"----- INFO: XGBoost Parameters -----\")\n",
    "print(param)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "4    0\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddi_clean.Label.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101992"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ddi_clean.Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: iter 1\n",
      "INFO: iter 10001\n",
      "INFO: iter 20001\n",
      "----- INFO: training data preprocess is done. -----\n",
      "----- INFO: XGBoost Parameters -----\n",
      "{'max_depth': 4, 'learning_rate': 0.1, 'eta': 0.3, 'verbosity': 3, 'objective': 'multi:softprob', 'eval_metric': ['merror', 'mlogloss'], 'num_class': 10}\n"
     ]
    }
   ],
   "source": [
    "# list of desired DDI Types\n",
    "desired_DDI = [0, # safe\n",
    "               #4, # The metabolism of #Drug2 can be increased when combined with #Drug1.\n",
    "               #6, # Drug1 may increase the anticoagulant activities of #Drug2.\n",
    "               15, # Drug1 may increase the cardiotoxic activities of #Drug2.\n",
    "               16, # Drug1 may increase the central nervous system depressant (CNS depressant) activities of #Drug2.\n",
    "               18, # Drug1 can cause an increase in the absorption of #Drug2 resulting in an increased serum concentration and potentially a worsening of adverse effects.\n",
    "               20, # Drug1 may increase the QTc-prolonging activities of #Drug2.\n",
    "               32, # Drug1 may increase the sedative activities of #Drug2.\n",
    "               54, # Drug1 may increase the bradycardic activities of #Drug2.\n",
    "               #67, # Drug1 can cause a decrease in the absorption of #Drug2 resulting in a reduced serum concentration and potentially a decrease in efficacy.\n",
    "               72, # Drug1 may decrease the excretion rate of #Drug2 which could result in a higher serum level.\n",
    "               76, # Drug1 may decrease the sedative activities of #Drug2.\n",
    "               80  # Drug1 may increase the hepatotoxic activities of #Drug2.\n",
    "              ]\n",
    "\n",
    "\n",
    "# parameters\n",
    "parameters = {\"learning_rate\"    : [0.01, 0.10, 0.25, 0.5, 1 ],\n",
    "    \"max_depth\"        : [ 3, 4, 5, 6, 7],\n",
    "    \"min_child_weight\" : [ 1, 3, 5, 7 ],\n",
    "    \"gamma\"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],\n",
    "    \"colsample_bytree\" : [ 0.3, 0.4, 0.5 , 0.7 ] }\n",
    "\n",
    "# load the data \n",
    "ddidata = pd.read_excel(\"DrugBank_known_ddi.xlsx\")\n",
    "interactiondict = pd.read_csv(\"Interaction_information.csv\")\n",
    "safe_drugs = pd.read_csv(\"safe_drug_combos.csv\")\n",
    "drug_similarity_feature = pd.read_csv(\"drug_similarity.csv\")\n",
    "drug_similarity = drug_similarity_feature.iloc[:, 1:len(drug_similarity_feature)+1]\n",
    "\n",
    "# filter ddidata for desired DDI types\n",
    "up_ddidata = ddidata[ddidata.Label.isin(desired_DDI)]\n",
    "new_ddidata = up_ddidata.copy()\n",
    "\n",
    "# convert types to int\n",
    "new_ddidata.drug1 = up_ddidata.drug1.str[2:].astype(int)\n",
    "new_ddidata.drug2 = up_ddidata.drug2.str[2:].astype(int)\n",
    "new_ddidata.Label = up_ddidata.Label\n",
    "\n",
    "\n",
    "# incorporate safe_drugs into new_ddidata with DDIType 0\n",
    "safe_drugs[\"Label\"] = 0\n",
    "\n",
    "frames = [safe_drugs, new_ddidata]\n",
    "ddi_df = pd.concat(frames)\n",
    "\n",
    "# create a DB to index dictionary from similarity dataset\n",
    "DB_to_index = {}\n",
    "i = 0\n",
    "for col in drug_similarity.columns:\n",
    "    DB_to_index[int(col[2:7])] = i\n",
    "    i = i + 1\n",
    "\n",
    "# filter output to only include DBs with similarity features\n",
    "ddi_df_output = ddi_df[ddi_df.drug1.isin(DB_to_index)]\n",
    "ddi_output = ddi_df_output[ddi_df_output.drug2.isin(DB_to_index)]\n",
    "\n",
    "# filter out the duplicate samples\n",
    "bool_series_to_delete = ddi_output[['drug1', 'drug2']].duplicated()\n",
    "ddi_clean = ddi_output[~bool_series_to_delete]\n",
    "\n",
    "\n",
    "# feature building \n",
    "# add similarity feature for each drug-drug pair\n",
    "n_similarity = 2159\n",
    "sim_array = np.empty([ddi_clean.shape[0], 2*n_similarity], dtype='float16')\n",
    "i = 0\n",
    "for index, (_, row) in enumerate(ddi_clean.iterrows()):\n",
    "    if index % 10000 == 0:\n",
    "        print(\"INFO: iter \" + str(index + 1))\n",
    "    drug1_index = DB_to_index[row[\"drug1\"]]\n",
    "    drug2_index = DB_to_index[row[\"drug2\"]]\n",
    "    feature_vec = np.hstack([drug_similarity.iloc[:,drug1_index],drug_similarity.iloc[:,drug2_index]])\n",
    "    sim_array[index, ] = feature_vec\n",
    "    \n",
    "# create input and output vectors for training\n",
    "\n",
    "X_data = sim_array\n",
    "y_data = np.array(ddi_clean.Label)\n",
    "\n",
    "# transform the y\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_data)\n",
    "encoded_Y = encoder.transform(y_data)\n",
    "\n",
    "#y_data = tf.keras.utils.to_categorical(encoded_Y)\n",
    "y_data = encoded_Y\n",
    "\n",
    "#X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X_data, y_data, test_size = 0.3\n",
    "#                                                                           , random_state = 1)\n",
    "#X_test, X_val, y_test, y_val = sklearn.model_selection.train_test_split(X_test, y_test, test_size = 0.5\n",
    "#                                                                       , random_state = 1)\n",
    "\n",
    "# dmatrix\n",
    "dtrain = xgb.DMatrix(X_data, label = y_data)\n",
    "#dtest = xgb.DMatrix(X_test, label = y_test)\n",
    "#dval = xgb.DMatrix(X_val, label = y_val)\n",
    "\n",
    "print(\"----- INFO: training data preprocess is done. -----\")\n",
    "# param\n",
    "max_depth = parameters[\"max_depth\"][idx_md]\n",
    "learning_rate = parameters[\"learning_rate\"][idx_lr]\n",
    "\n",
    "\n",
    "param = {\n",
    "    'max_depth': max_depth,  # the maximum depth of each tree\n",
    "    'learning_rate': learning_rate,\n",
    "    'eta': 0.3,  # the training step for each iteration\n",
    "    #'silent': 3,  # logging mode - quiet\n",
    "    'verbosity': 3,  # debug mode\n",
    "    'objective': 'multi:softprob',  # error evaluation for multiclass training\n",
    "    'eval_metric': ['merror', 'mlogloss'],\n",
    "    'num_class': 10}  # the number of classes that exist in this datset\n",
    "num_round = 2 # the number of training iterations\n",
    "\n",
    "print(\"----- INFO: XGBoost Parameters -----\")\n",
    "print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "XGBoostError",
     "evalue": "[03:05:54] /workspace/src/objective/multiclass_obj.cu:115: SoftmaxMultiClassObj: label must be in [0, num_class).\nStack trace:\n  [bt] (0) /home/zy92/anaconda3/lib/python3.7/site-packages/xgboost/./lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x54) [0x2b1fa5f2b614]\n  [bt] (1) /home/zy92/anaconda3/lib/python3.7/site-packages/xgboost/./lib/libxgboost.so(xgboost::obj::SoftmaxMultiClassObj::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*)+0x586) [0x2b1fa61a3ee6]\n  [bt] (2) /home/zy92/anaconda3/lib/python3.7/site-packages/xgboost/./lib/libxgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, xgboost::DMatrix*)+0x376) [0x2b1fa60179a6]\n  [bt] (3) /home/zy92/anaconda3/lib/python3.7/site-packages/xgboost/./lib/libxgboost.so(XGBoosterUpdateOneIter+0x29) [0x2b1fa5f1a639]\n  [bt] (4) /home/zy92/anaconda3/lib/python3.7/lib-dynload/../../libffi.so.6(ffi_call_unix64+0x4c) [0x2b1f769e9ec0]\n  [bt] (5) /home/zy92/anaconda3/lib/python3.7/lib-dynload/../../libffi.so.6(ffi_call+0x22d) [0x2b1f769e987d]\n  [bt] (6) /home/zy92/anaconda3/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x2b1f769d0ede]\n  [bt] (7) /home/zy92/anaconda3/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12914) [0x2b1f769d1914]\n  [bt] (8) /home/zy92/anaconda3/bin/python3.7(_PyObject_FastCallKeywords+0x49b) [0x562c214068fb]\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mXGBoostError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-ec859a8d500e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m                 \u001b[0mnfold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                 \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                 \u001b[0mearly_stopping_rounds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m                 \u001b[0;31m#evals = [(dtest,'test'), (dtrain,'train')],\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                 \u001b[0;31m#evals_result = history_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mcv\u001b[0;34m(params, dtrain, num_boost_round, nfold, stratified, folds, metrics, obj, feval, maximize, early_stopping_rounds, fpreproc, as_pandas, verbose_eval, show_stdv, seed, callbacks, shuffle)\u001b[0m\n\u001b[1;32m    493\u001b[0m                            evaluation_result_list=None))\n\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfold\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcvfolds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m             \u001b[0mfold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    496\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maggcv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcvfolds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, iteration, fobj)\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;34m\"\"\"\"Update the boosters for one iteration\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1246\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,\n\u001b[1;32m   1247\u001b[0m                                                     \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1248\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m   1249\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1250\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m_check_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    187\u001b[0m     \"\"\"\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mXGBoostError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBGetLastError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mXGBoostError\u001b[0m: [03:05:54] /workspace/src/objective/multiclass_obj.cu:115: SoftmaxMultiClassObj: label must be in [0, num_class).\nStack trace:\n  [bt] (0) /home/zy92/anaconda3/lib/python3.7/site-packages/xgboost/./lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x54) [0x2b1fa5f2b614]\n  [bt] (1) /home/zy92/anaconda3/lib/python3.7/site-packages/xgboost/./lib/libxgboost.so(xgboost::obj::SoftmaxMultiClassObj::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*)+0x586) [0x2b1fa61a3ee6]\n  [bt] (2) /home/zy92/anaconda3/lib/python3.7/site-packages/xgboost/./lib/libxgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, xgboost::DMatrix*)+0x376) [0x2b1fa60179a6]\n  [bt] (3) /home/zy92/anaconda3/lib/python3.7/site-packages/xgboost/./lib/libxgboost.so(XGBoosterUpdateOneIter+0x29) [0x2b1fa5f1a639]\n  [bt] (4) /home/zy92/anaconda3/lib/python3.7/lib-dynload/../../libffi.so.6(ffi_call_unix64+0x4c) [0x2b1f769e9ec0]\n  [bt] (5) /home/zy92/anaconda3/lib/python3.7/lib-dynload/../../libffi.so.6(ffi_call+0x22d) [0x2b1f769e987d]\n  [bt] (6) /home/zy92/anaconda3/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x2b1f769d0ede]\n  [bt] (7) /home/zy92/anaconda3/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12914) [0x2b1f769d1914]\n  [bt] (8) /home/zy92/anaconda3/bin/python3.7(_PyObject_FastCallKeywords+0x49b) [0x562c214068fb]\n\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "history_dict = {}\n",
    "bst = xgb.cv(param, \n",
    "                dtrain, \n",
    "                num_round,\n",
    "                nfold = 5,\n",
    "                seed =0,\n",
    "                early_stopping_rounds = 5,\n",
    "                #evals = [(dtest,'test'), (dtrain,'train')],\n",
    "                #evals_result = history_dict\n",
    "                )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train-merror-mean</th>\n",
       "      <th>train-merror-std</th>\n",
       "      <th>train-mlogloss-mean</th>\n",
       "      <th>train-mlogloss-std</th>\n",
       "      <th>test-merror-mean</th>\n",
       "      <th>test-merror-std</th>\n",
       "      <th>test-mlogloss-mean</th>\n",
       "      <th>test-mlogloss-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.233878</td>\n",
       "      <td>0.003391</td>\n",
       "      <td>2.618907</td>\n",
       "      <td>0.00159</td>\n",
       "      <td>0.237205</td>\n",
       "      <td>0.004361</td>\n",
       "      <td>2.62549</td>\n",
       "      <td>0.002341</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train-merror-mean  train-merror-std  train-mlogloss-mean  \\\n",
       "0           0.233878          0.003391             2.618907   \n",
       "\n",
       "   train-mlogloss-std  test-merror-mean  test-merror-std  test-mlogloss-mean  \\\n",
       "0             0.00159          0.237205         0.004361             2.62549   \n",
       "\n",
       "   test-mlogloss-std  \n",
       "0           0.002341  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cv() got an unexpected keyword argument 'evals'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-09e423be055c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m                 \u001b[0mearly_stopping_rounds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                 \u001b[0mevals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                 \u001b[0mevals_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m                 )\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cv() got an unexpected keyword argument 'evals'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# testing\n",
    "preds = bst.predict(dtest)\n",
    "best_preds = np.asarray([np.argmax(line) for line in preds])\n",
    "preds = bst.predict(dtest)\n",
    "best_preds = np.asarray([np.argmax(line) for line in preds])\n",
    "\n",
    "# save the object \n",
    "print(\"----- INFO: saving results -----\")\n",
    "output_dir = \"/ysm-gpfs/pi/zhao/zy92/projects/ddipred/ddi_pred/code/xgboost/cv_output/\"\n",
    "history_path = output_dir + \"_lr_\" + learning_rate + \"_md_\" + max_depth + \"_history_dict.pickle\"\n",
    "model_path = output_dir + \"_lr_\" + learning_rate + \"_md_\" + max_depth + \"_xgb.pickle\"\n",
    "pickle.dump(history_dict, history_path)\n",
    "pickle.dump(bst, model_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:39:33] DEBUG: /workspace/src/gbm/gbtree.cc:146: Using tree method: 2\n",
      "[19:39:39] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 16 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[19:39:43] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 4 extra nodes, 0 pruned nodes, max_depth=2\n",
      "[19:39:49] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 20 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[19:39:56] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 12 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[19:40:02] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 16 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[19:40:08] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 20 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[19:40:14] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 10 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[19:40:20] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 6 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[19:40:23] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 2 extra nodes, 0 pruned nodes, max_depth=1\n",
      "[19:40:29] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 10 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[19:40:36] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 14 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[19:40:42] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 18 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[19:40:48] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 16 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[19:40:54] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 26 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[19:41:01] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 18 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[19:41:05] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 4 extra nodes, 0 pruned nodes, max_depth=2\n",
      "[19:41:09] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 4 extra nodes, 0 pruned nodes, max_depth=2\n",
      "[19:41:15] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 10 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[19:41:21] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 22 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[19:41:24] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 2 extra nodes, 0 pruned nodes, max_depth=1\n",
      "[19:41:30] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 14 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[19:41:36] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 10 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[19:41:42] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 16 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[19:41:46] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 4 extra nodes, 0 pruned nodes, max_depth=2\n",
      "[19:41:51] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 4 extra nodes, 0 pruned nodes, max_depth=2\n",
      "[19:41:55] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 4 extra nodes, 0 pruned nodes, max_depth=2\n",
      "[19:41:58] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 2 extra nodes, 0 pruned nodes, max_depth=1\n",
      "[19:42:05] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 30 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[19:42:09] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 4 extra nodes, 0 pruned nodes, max_depth=2\n",
      "[19:42:15] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 12 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[19:42:19] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 4 extra nodes, 0 pruned nodes, max_depth=2\n",
      "[19:42:26] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 24 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[19:42:31] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 10 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[19:42:36] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 4 extra nodes, 0 pruned nodes, max_depth=2\n",
      "[19:42:42] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 14 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[19:42:49] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 10 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[19:42:55] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 18 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[19:43:02] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[19:43:07] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 18 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[19:43:12] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 4 extra nodes, 0 pruned nodes, max_depth=2\n",
      "[19:43:17] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 6 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[19:43:23] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 12 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[19:43:29] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 12 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[0]\tvalidation-merror:0.23910\tvalidation-mlogloss:2.62141\ttest-merror:0.23485\ttest-mlogloss:2.61461\ttrain-merror:0.23089\ttrain-mlogloss:2.61142\n",
      "Multiple eval metrics have been passed: 'train-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until train-mlogloss hasn't improved in 5 rounds.\n",
      "[19:43:54] ======== Monitor: Learner ========\n",
      "[19:43:54] Configure: 0.001421s, 1 calls @ 1421us\n",
      "\n",
      "[19:43:54] EvalOneIter: 0.084271s, 1 calls @ 84271us\n",
      "\n",
      "[19:43:54] GetGradient: 0.054435s, 1 calls @ 54435us\n",
      "\n",
      "[19:43:54] PredictRaw: 17.0338s, 1 calls @ 17033792us\n",
      "\n",
      "[19:43:54] UpdateOneIter: 280.771s, 1 calls @ 280771086us\n",
      "\n",
      "[19:43:54] ======== Monitor: GBTree ========\n",
      "[19:43:54] BoostNewTrees: 239.003s, 1 calls @ 239003271us\n",
      "\n",
      "[19:43:54] CommitModel: 24.6774s, 1 calls @ 24677409us\n",
      "\n",
      "----- INFO: saving results -----\n"
     ]
    }
   ],
   "source": [
    "param = {\n",
    "    'max_depth': max_depth,  # the maximum depth of each tree\n",
    "    'learning_rate': learning_rate,\n",
    "    'eta': 0.3,  # the training step for each iteration\n",
    "    #'silent': 3,  # logging mode - quiet\n",
    "    'verbosity': 3,  # debug mode\n",
    "    'objective': 'multi:softprob',  # error evaluation for multiclass training\n",
    "    'eval_metric': ['merror', 'mlogloss'],\n",
    "    'num_class': 43}  # the number of classes that exist in this datset\n",
    "num_round = 1  # the number of training iterations\n",
    "\n",
    "history_dict = {}\n",
    "bst = xgb.train(param, \n",
    "                dtrain, \n",
    "                num_round,\n",
    "                early_stopping_rounds = 5,\n",
    "                evals = [(dval, 'validation'), (dtest,'test'), (dtrain,'train')],\n",
    "                #evals = [(dtest,'test'), (dtrain,'train')],\n",
    "                evals_result = history_dict\n",
    "                )\n",
    "\n",
    "\n",
    "\n",
    "# testing\n",
    "#preds = bst.predict(dtest)\n",
    "#best_preds = np.asarray([np.argmax(line) for line in preds])\n",
    "#preds = bst.predict(dtest)\n",
    "#best_preds = np.asarray([np.argmax(line) for line in preds])\n",
    "\n",
    "# save the object \n",
    "print(\"----- INFO: saving results -----\")\n",
    "output_dir = \"/ysm-gpfs/pi/zhao/zy92/projects/ddipred/ddi_pred/code/xgboost/output/\"\n",
    "history_path = output_dir + \"_lr_\" + str(learning_rate) + \"_md_\" + str(max_depth) + \"_history_dict.pickle\"\n",
    "model_path = output_dir + \"_lr_\" + str(learning_rate) + \"_md_\" + str(max_depth) + \"_xgb.pickle\"\n",
    "history_fo = open(history_path, \"wb\")\n",
    "model_fo = open(model_path, \"wb\")\n",
    "pickle.dump(history_dict, history_fo)\n",
    "pickle.dump(bst, model_fo)\n",
    "history_fo.close()\n",
    "model_fo.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "xgboost.core.DMatrix"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dtrain)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
