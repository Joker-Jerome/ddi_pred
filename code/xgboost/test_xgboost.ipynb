{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:34:51] ======== Monitor: Learner ========\n",
      "[18:34:51] ======== Monitor: GBTree ========\n",
      "[18:34:51] ======== Monitor: Learner ========\n",
      "[18:34:51] Configure: 0.001375s, 1 calls @ 1375us\n",
      "\n",
      "[18:34:51] WARNING: /workspace/src/common/timer.cc:88: Timer for EvalOneIter did not get stopped properly.\n",
      "[18:34:51] GetGradient: 0.060069s, 1 calls @ 60069us\n",
      "\n",
      "[18:34:51] PredictRaw: 18.0722s, 1 calls @ 18072213us\n",
      "\n",
      "[18:34:51] UpdateOneIter: 276.569s, 1 calls @ 276569098us\n",
      "\n",
      "[18:34:51] ======== Monitor: GBTree ========\n",
      "[18:34:51] BoostNewTrees: 234.052s, 1 calls @ 234051795us\n",
      "\n",
      "[18:34:51] CommitModel: 24.3829s, 1 calls @ 24382897us\n",
      "\n",
      "[18:34:51] ======== Monitor: Learner ========\n",
      "[18:34:51] Configure: 0.001229s, 1 calls @ 1229us\n",
      "\n",
      "[18:34:51] WARNING: /workspace/src/common/timer.cc:88: Timer for EvalOneIter did not get stopped properly.\n",
      "[18:34:51] GetGradient: 0.060403s, 1 calls @ 60403us\n",
      "\n",
      "[18:34:51] PredictRaw: 16.6614s, 1 calls @ 16661366us\n",
      "\n",
      "[18:34:51] UpdateOneIter: 304.127s, 1 calls @ 304126857us\n",
      "\n",
      "[18:34:51] ======== Monitor: GBTree ========\n",
      "[18:34:51] BoostNewTrees: 262.78s, 1 calls @ 262779527us\n",
      "\n",
      "[18:34:51] CommitModel: 24.624s, 1 calls @ 24623981us\n",
      "\n",
      "INFO: iter 1\n",
      "INFO: iter 10001\n",
      "INFO: iter 20001\n",
      "INFO: iter 30001\n",
      "INFO: iter 40001\n",
      "INFO: iter 50001\n",
      "INFO: iter 60001\n",
      "INFO: iter 70001\n",
      "INFO: iter 80001\n",
      "INFO: iter 90001\n",
      "INFO: iter 100001\n",
      "----- INFO: training data preprocess is done. -----\n",
      "----- INFO: XGBoost Parameters -----\n",
      "{'max_depth': 4, 'learning_rate': 0.1, 'eta': 0.3, 'verbosity': 3, 'objective': 'multi:softprob', 'eval_metric': ['mae', 'mlogloss'], 'num_class': 43}\n",
      "[18:35:48] DEBUG: /workspace/src/gbm/gbtree.cc:146: Using tree method: 2\n",
      "[18:36:15] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 16 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[18:36:21] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 4 extra nodes, 0 pruned nodes, max_depth=2\n",
      "[18:36:29] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 20 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[18:36:37] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 12 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[18:36:45] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 16 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[18:36:54] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 20 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[18:37:02] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 10 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[18:37:10] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 6 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:37:14] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 2 extra nodes, 0 pruned nodes, max_depth=1\n",
      "[18:37:22] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 10 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[18:37:32] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 14 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[18:37:41] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 18 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[18:37:51] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 16 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[18:38:01] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 26 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[18:38:11] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 18 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[18:38:18] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 4 extra nodes, 0 pruned nodes, max_depth=2\n",
      "[18:38:25] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 4 extra nodes, 0 pruned nodes, max_depth=2\n",
      "[18:38:35] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 10 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[18:38:44] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 22 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[18:38:50] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 2 extra nodes, 0 pruned nodes, max_depth=1\n",
      "[18:38:59] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 14 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[18:39:09] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 10 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:39:18] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 16 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[18:39:24] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 4 extra nodes, 0 pruned nodes, max_depth=2\n",
      "[18:39:30] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 4 extra nodes, 0 pruned nodes, max_depth=2\n",
      "[18:39:36] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 4 extra nodes, 0 pruned nodes, max_depth=2\n",
      "[18:39:40] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 2 extra nodes, 0 pruned nodes, max_depth=1\n",
      "[18:39:48] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 30 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[18:39:54] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 4 extra nodes, 0 pruned nodes, max_depth=2\n",
      "[18:40:02] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 12 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[18:40:07] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 4 extra nodes, 0 pruned nodes, max_depth=2\n",
      "[18:40:16] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 24 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[18:40:23] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 10 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[18:40:29] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 4 extra nodes, 0 pruned nodes, max_depth=2\n",
      "[18:40:37] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 14 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[18:40:45] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 10 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[18:40:54] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 18 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[18:41:02] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:41:09] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 18 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[18:41:15] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 4 extra nodes, 0 pruned nodes, max_depth=2\n",
      "[18:41:22] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 6 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:41:30] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 12 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[18:41:38] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 12 extra nodes, 0 pruned nodes, max_depth=4\n"
     ]
    },
    {
     "ename": "XGBoostError",
     "evalue": "[18:41:59] /workspace/src/metric/elementwise_metric.cu:332: Check failed: preds.Size() == info.labels_.Size() (657857 vs. 15299) : label and prediction size not match, hint: use merror or mlogloss for multi-class classification\nStack trace:\n  [bt] (0) /home/zy92/anaconda3/lib/python3.7/site-packages/xgboost/./lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x54) [0x2ae34dd42614]\n  [bt] (1) /home/zy92/anaconda3/lib/python3.7/site-packages/xgboost/./lib/libxgboost.so(xgboost::metric::EvalEWiseBase<xgboost::metric::EvalRowMAE>::Eval(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, bool)+0x15a) [0x2ae34dfa8c6a]\n  [bt] (2) /home/zy92/anaconda3/lib/python3.7/site-packages/xgboost/./lib/libxgboost.so(xgboost::LearnerImpl::EvalOneIter(int, std::vector<xgboost::DMatrix*, std::allocator<xgboost::DMatrix*> > const&, std::vector<std::string, std::allocator<std::string> > const&)+0x419) [0x2ae34de2efb9]\n  [bt] (3) /home/zy92/anaconda3/lib/python3.7/site-packages/xgboost/./lib/libxgboost.so(XGBoosterEvalOneIter+0x363) [0x2ae34dd35a23]\n  [bt] (4) /home/zy92/anaconda3/lib/python3.7/lib-dynload/../../libffi.so.6(ffi_call_unix64+0x4c) [0x2ae31e7fdec0]\n  [bt] (5) /home/zy92/anaconda3/lib/python3.7/lib-dynload/../../libffi.so.6(ffi_call+0x22d) [0x2ae31e7fd87d]\n  [bt] (6) /home/zy92/anaconda3/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x2ae31e7e4ede]\n  [bt] (7) /home/zy92/anaconda3/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12914) [0x2ae31e7e5914]\n  [bt] (8) /home/zy92/anaconda3/bin/python3.7(_PyObject_FastCallKeywords+0x49b) [0x556a08c108fb]\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mXGBoostError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-3c0a4db9227a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    152\u001b[0m                 \u001b[0mearly_stopping_rounds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m                 \u001b[0mevals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m                 \u001b[0mevals_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m                 )\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m    207\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;31m# check evaluation result.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m             \u001b[0mbst_eval_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst_eval_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSTRING_TYPES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbst_eval_set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36meval_set\u001b[0;34m(self, evals, iteration, feval)\u001b[0m\n\u001b[1;32m   1312\u001b[0m                                               \u001b[0mdmats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevnames\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1313\u001b[0m                                               \u001b[0mc_bst_ulong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1314\u001b[0;31m                                               ctypes.byref(msg)))\n\u001b[0m\u001b[1;32m   1315\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfeval\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m_check_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    187\u001b[0m     \"\"\"\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mXGBoostError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBGetLastError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mXGBoostError\u001b[0m: [18:41:59] /workspace/src/metric/elementwise_metric.cu:332: Check failed: preds.Size() == info.labels_.Size() (657857 vs. 15299) : label and prediction size not match, hint: use merror or mlogloss for multi-class classification\nStack trace:\n  [bt] (0) /home/zy92/anaconda3/lib/python3.7/site-packages/xgboost/./lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x54) [0x2ae34dd42614]\n  [bt] (1) /home/zy92/anaconda3/lib/python3.7/site-packages/xgboost/./lib/libxgboost.so(xgboost::metric::EvalEWiseBase<xgboost::metric::EvalRowMAE>::Eval(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, bool)+0x15a) [0x2ae34dfa8c6a]\n  [bt] (2) /home/zy92/anaconda3/lib/python3.7/site-packages/xgboost/./lib/libxgboost.so(xgboost::LearnerImpl::EvalOneIter(int, std::vector<xgboost::DMatrix*, std::allocator<xgboost::DMatrix*> > const&, std::vector<std::string, std::allocator<std::string> > const&)+0x419) [0x2ae34de2efb9]\n  [bt] (3) /home/zy92/anaconda3/lib/python3.7/site-packages/xgboost/./lib/libxgboost.so(XGBoosterEvalOneIter+0x363) [0x2ae34dd35a23]\n  [bt] (4) /home/zy92/anaconda3/lib/python3.7/lib-dynload/../../libffi.so.6(ffi_call_unix64+0x4c) [0x2ae31e7fdec0]\n  [bt] (5) /home/zy92/anaconda3/lib/python3.7/lib-dynload/../../libffi.so.6(ffi_call+0x22d) [0x2ae31e7fd87d]\n  [bt] (6) /home/zy92/anaconda3/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x2ae31e7e4ede]\n  [bt] (7) /home/zy92/anaconda3/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12914) [0x2ae31e7e5914]\n  [bt] (8) /home/zy92/anaconda3/bin/python3.7(_PyObject_FastCallKeywords+0x49b) [0x556a08c108fb]\n\n"
     ]
    }
   ],
   "source": [
    "#!/bin/python\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "import sklearn.model_selection\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle\n",
    "import xgboost as xgb\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, train_test_split, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, mean_squared_error, precision_score, accuracy_score\n",
    "\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# argument parsing \n",
    "#parser = argparse.ArgumentParser(description='Process some integers.')\n",
    "#parser.add_argument('integers', metavar='N', type=int, nargs='+',\n",
    "#                   help='an integer for the accumulator')\n",
    "#parser.add_argument('--sum', dest='accumulate', action='store_const',\n",
    "#                   const=sum, default=max,\n",
    "#                   help='sum the integers (default: find the max)')\n",
    "\n",
    "#args = parser.parse_args()\n",
    "#idx_lr = sys.argv[1]\n",
    "idx_lr = 1\n",
    "#idx_md = sys.argv[2]\n",
    "idx_md = 1\n",
    "\n",
    "# set seed \n",
    "rng = np.random.RandomState(1)\n",
    "\n",
    "# set directory\n",
    "os.getcwd()\n",
    "os.chdir(\"/ysm-gpfs/pi/zhao/zy92/projects/ddipred/ddi_pred/data\")\n",
    "\n",
    "# list of desired DDI Types\n",
    "desired_DDI = [0, 1, 2, 3, 4, 5, 6, 7, 15, 16, 17, 18, 19, 20, 21, 22, 26, 28, 30, 31, 32, 38, 40, 41, 43, 44, 45,\n",
    "               49, 50, 51, 52, 54, 55, 62, 67, 68, 72, 74, 76, 78, 79, 80, 81]\n",
    "\n",
    "\n",
    "# parameters\n",
    "parameters = {\"learning_rate\"    : [0.01, 0.10, 0.25, 0.5, 1 ],\n",
    "    \"max_depth\"        : [ 3, 4, 5, 6, 7],\n",
    "    \"min_child_weight\" : [ 1, 3, 5, 7 ],\n",
    "    \"gamma\"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],\n",
    "    \"colsample_bytree\" : [ 0.3, 0.4, 0.5 , 0.7 ] }\n",
    "\n",
    "# load the data \n",
    "ddidata = pd.read_excel(\"DrugBank_known_ddi.xlsx\")\n",
    "interactiondict = pd.read_csv(\"Interaction_information.csv\")\n",
    "safe_drugs = pd.read_csv(\"safe_drug_combos.csv\")\n",
    "drug_similarity_feature = pd.read_csv(\"drug_similarity.csv\")\n",
    "drug_similarity = drug_similarity_feature.iloc[:, 1:len(drug_similarity_feature)+1]\n",
    "\n",
    "# filter ddidata for desired DDI types\n",
    "up_ddidata = ddidata[ddidata.Label.isin(desired_DDI)]\n",
    "new_ddidata = up_ddidata.copy()\n",
    "\n",
    "# convert types to int\n",
    "new_ddidata.drug1 = up_ddidata.drug1.str[2:].astype(int)\n",
    "new_ddidata.drug2 = up_ddidata.drug2.str[2:].astype(int)\n",
    "new_ddidata.Label = up_ddidata.Label\n",
    "\n",
    "\n",
    "# incorporate safe_drugs into new_ddidata with DDIType 0\n",
    "safe_drugs[\"Label\"] = 0\n",
    "\n",
    "frames = [safe_drugs, new_ddidata]\n",
    "ddi_df = pd.concat(frames)\n",
    "\n",
    "# create a DB to index dictionary from similarity dataset\n",
    "DB_to_index = {}\n",
    "i = 0\n",
    "for col in drug_similarity.columns:\n",
    "    DB_to_index[int(col[2:7])] = i\n",
    "    i = i + 1\n",
    "\n",
    "# filter output to only include DBs with similarity features\n",
    "ddi_df_output = ddi_df[ddi_df.drug1.isin(DB_to_index)]\n",
    "ddi_output = ddi_df_output[ddi_df_output.drug2.isin(DB_to_index)]\n",
    "\n",
    "# filter out the duplicate samples\n",
    "bool_series_to_delete = ddi_output[['drug1', 'drug2']].duplicated()\n",
    "ddi_clean = ddi_output[~bool_series_to_delete]\n",
    "\n",
    "\n",
    "# feature building \n",
    "# add similarity feature for each drug-drug pair\n",
    "n_similarity = 2159\n",
    "sim_array = np.empty([ddi_clean.shape[0], 2*n_similarity], dtype='float16')\n",
    "i = 0\n",
    "for index, (_, row) in enumerate(ddi_clean.iterrows()):\n",
    "    if index % 10000 == 0:\n",
    "        print(\"INFO: iter \" + str(index + 1))\n",
    "    drug1_index = DB_to_index[row[\"drug1\"]]\n",
    "    drug2_index = DB_to_index[row[\"drug2\"]]\n",
    "    feature_vec = np.hstack([drug_similarity.iloc[:,drug1_index],drug_similarity.iloc[:,drug2_index]])\n",
    "    sim_array[index, ] = feature_vec\n",
    "    \n",
    "# create input and output vectors for training\n",
    "\n",
    "X_data = sim_array\n",
    "y_data = np.array(ddi_clean.Label)\n",
    "\n",
    "# transform the y\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_data)\n",
    "encoded_Y = encoder.transform(y_data)\n",
    "\n",
    "#y_data = tf.keras.utils.to_categorical(encoded_Y)\n",
    "y_data = encoded_Y\n",
    "\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X_data, y_data, test_size = 0.3\n",
    "                                                                           , random_state = 1)\n",
    "X_test, X_val, y_test, y_val = sklearn.model_selection.train_test_split(X_test, y_test, test_size = 0.5\n",
    "                                                                       , random_state = 1)\n",
    "\n",
    "# dmatrix\n",
    "dtrain = xgb.DMatrix(X_train, label = y_train)\n",
    "dtest = xgb.DMatrix(X_test, label = y_test)\n",
    "dval = xgb.DMatrix(X_val, label = y_val)\n",
    "\n",
    "print(\"----- INFO: training data preprocess is done. -----\")\n",
    "# param\n",
    "max_depth = parameters[\"max_depth\"][idx_md]\n",
    "learning_rate = parameters[\"learning_rate\"][idx_lr]\n",
    "\n",
    "\n",
    "param = {\n",
    "    'max_depth': max_depth,  # the maximum depth of each tree\n",
    "    'learning_rate': learning_rate,\n",
    "    'eta': 0.3,  # the training step for each iteration\n",
    "    #'silent': 3,  # logging mode - quiet\n",
    "    'verbosity': 3,  # debug mode\n",
    "    'objective': 'multi:softprob',  # error evaluation for multiclass training\n",
    "    'eval_metric': ['mae', 'mlogloss'],\n",
    "    'num_class': 43}  # the number of classes that exist in this datset\n",
    "num_round = 1  # the number of training iterations\n",
    "\n",
    "print(\"----- INFO: XGBoost Parameters -----\")\n",
    "print(param)\n",
    "# training\n",
    "history_dict = {}\n",
    "bst = xgb.train(param, \n",
    "                dtrain, \n",
    "                num_round,\n",
    "                early_stopping_rounds = 5,\n",
    "                evals = [(dtest,'test'), (dtrain,'train')],\n",
    "                evals_result = history_dict\n",
    "                )\n",
    "\n",
    "\n",
    "\n",
    "# testing\n",
    "preds = bst.predict(dtest)\n",
    "best_preds = np.asarray([np.argmax(line) for line in preds])\n",
    "preds = bst.predict(dtest)\n",
    "best_preds = np.asarray([np.argmax(line) for line in preds])\n",
    "\n",
    "# save the object \n",
    "print(\"----- INFO: saving results -----\")\n",
    "output_dir = \"/ysm-gpfs/pi/zhao/zy92/projects/ddipred/ddi_pred/code/xgboost/output/\"\n",
    "history_path = output_dir + \"_lr_\" + learning_rate + \"_md_\" + max_depth + \"_history_dict.pickle\"\n",
    "model_path = output_dir + \"_lr_\" + learning_rate + \"_md_\" + max_depth + \"_xgb.pickle\"\n",
    "pickle.dump(history_dict, history_path)\n",
    "pickle.dump(bst, model_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:39:33] DEBUG: /workspace/src/gbm/gbtree.cc:146: Using tree method: 2\n",
      "[19:39:39] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 16 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[19:39:43] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 4 extra nodes, 0 pruned nodes, max_depth=2\n",
      "[19:39:49] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 20 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[19:39:56] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 12 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[19:40:02] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 16 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[19:40:08] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 20 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[19:40:14] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 10 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[19:40:20] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 6 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[19:40:23] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 2 extra nodes, 0 pruned nodes, max_depth=1\n",
      "[19:40:29] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 10 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[19:40:36] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 14 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[19:40:42] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 18 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[19:40:48] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 16 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[19:40:54] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 26 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[19:41:01] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 18 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[19:41:05] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 4 extra nodes, 0 pruned nodes, max_depth=2\n",
      "[19:41:09] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 4 extra nodes, 0 pruned nodes, max_depth=2\n",
      "[19:41:15] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 10 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[19:41:21] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 22 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[19:41:24] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 2 extra nodes, 0 pruned nodes, max_depth=1\n",
      "[19:41:30] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 14 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[19:41:36] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 10 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[19:41:42] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 16 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[19:41:46] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 4 extra nodes, 0 pruned nodes, max_depth=2\n",
      "[19:41:51] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 4 extra nodes, 0 pruned nodes, max_depth=2\n",
      "[19:41:55] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 4 extra nodes, 0 pruned nodes, max_depth=2\n",
      "[19:41:58] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 2 extra nodes, 0 pruned nodes, max_depth=1\n",
      "[19:42:05] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 30 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[19:42:09] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 4 extra nodes, 0 pruned nodes, max_depth=2\n",
      "[19:42:15] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 12 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[19:42:19] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 4 extra nodes, 0 pruned nodes, max_depth=2\n",
      "[19:42:26] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 24 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[19:42:31] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 10 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[19:42:36] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 4 extra nodes, 0 pruned nodes, max_depth=2\n",
      "[19:42:42] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 14 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[19:42:49] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 10 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[19:42:55] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 18 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[19:43:02] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[19:43:07] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 18 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[19:43:12] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 4 extra nodes, 0 pruned nodes, max_depth=2\n",
      "[19:43:17] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 6 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[19:43:23] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 12 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[19:43:29] INFO: /workspace/src/tree/updater_prune.cc:89: tree pruning end, 12 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[0]\tvalidation-merror:0.23910\tvalidation-mlogloss:2.62141\ttest-merror:0.23485\ttest-mlogloss:2.61461\ttrain-merror:0.23089\ttrain-mlogloss:2.61142\n",
      "Multiple eval metrics have been passed: 'train-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until train-mlogloss hasn't improved in 5 rounds.\n",
      "[19:43:54] ======== Monitor: Learner ========\n",
      "[19:43:54] Configure: 0.001421s, 1 calls @ 1421us\n",
      "\n",
      "[19:43:54] EvalOneIter: 0.084271s, 1 calls @ 84271us\n",
      "\n",
      "[19:43:54] GetGradient: 0.054435s, 1 calls @ 54435us\n",
      "\n",
      "[19:43:54] PredictRaw: 17.0338s, 1 calls @ 17033792us\n",
      "\n",
      "[19:43:54] UpdateOneIter: 280.771s, 1 calls @ 280771086us\n",
      "\n",
      "[19:43:54] ======== Monitor: GBTree ========\n",
      "[19:43:54] BoostNewTrees: 239.003s, 1 calls @ 239003271us\n",
      "\n",
      "[19:43:54] CommitModel: 24.6774s, 1 calls @ 24677409us\n",
      "\n",
      "----- INFO: saving results -----\n"
     ]
    }
   ],
   "source": [
    "param = {\n",
    "    'max_depth': max_depth,  # the maximum depth of each tree\n",
    "    'learning_rate': learning_rate,\n",
    "    'eta': 0.3,  # the training step for each iteration\n",
    "    #'silent': 3,  # logging mode - quiet\n",
    "    'verbosity': 3,  # debug mode\n",
    "    'objective': 'multi:softprob',  # error evaluation for multiclass training\n",
    "    'eval_metric': ['merror', 'mlogloss'],\n",
    "    'num_class': 43}  # the number of classes that exist in this datset\n",
    "num_round = 1  # the number of training iterations\n",
    "\n",
    "history_dict = {}\n",
    "bst = xgb.train(param, \n",
    "                dtrain, \n",
    "                num_round,\n",
    "                early_stopping_rounds = 5,\n",
    "                evals = [(dval, 'validation'), (dtest,'test'), (dtrain,'train')],\n",
    "                #evals = [(dtest,'test'), (dtrain,'train')],\n",
    "                evals_result = history_dict\n",
    "                )\n",
    "\n",
    "\n",
    "\n",
    "# testing\n",
    "#preds = bst.predict(dtest)\n",
    "#best_preds = np.asarray([np.argmax(line) for line in preds])\n",
    "#preds = bst.predict(dtest)\n",
    "#best_preds = np.asarray([np.argmax(line) for line in preds])\n",
    "\n",
    "# save the object \n",
    "print(\"----- INFO: saving results -----\")\n",
    "output_dir = \"/ysm-gpfs/pi/zhao/zy92/projects/ddipred/ddi_pred/code/xgboost/output/\"\n",
    "history_path = output_dir + \"_lr_\" + str(learning_rate) + \"_md_\" + str(max_depth) + \"_history_dict.pickle\"\n",
    "model_path = output_dir + \"_lr_\" + str(learning_rate) + \"_md_\" + str(max_depth) + \"_xgb.pickle\"\n",
    "history_fo = open(history_path, \"wb\")\n",
    "model_fo = open(model_path, \"wb\")\n",
    "pickle.dump(history_dict, history_fo)\n",
    "pickle.dump(bst, model_fo)\n",
    "history_fo.close()\n",
    "model_fo.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "xgboost.core.DMatrix"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dtrain)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
