{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "import argparse\n",
    "import sys\n",
    "from sklearn.model_selection import KFold\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### input files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_file = \"mini_matrix.txt\"\n",
    "#x_file = argv[1]\n",
    "\n",
    "y_file = \"mini_label.txt\"\n",
    "#y_file = argv[2]\n",
    "\n",
    "# fold \n",
    "#fold = int(argv[3])\n",
    "fold = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get data\n",
    "x_data = np.loadtxt(x_file, delimiter = \"\\t\")\n",
    "x_data = x_data[:, 6:]\n",
    "y_data = np.loadtxt(y_file, delimiter = \"\\t\")\n",
    "y_data = np.column_stack((y_data, 1 - y_data))\n",
    "\n",
    "# spliting\n",
    "kf = KFold(n_splits = 5, random_state = 1)\n",
    "kf.get_n_splits(x_data)\n",
    "k_list = []\n",
    "for train_index, test_index in kf.split(x_data):\n",
    "    k_list.append([train_index, test_index])    \n",
    "cur_index = k_list[fold - 1]\n",
    "\n",
    "# training data\n",
    "x_train = x_data[cur_index[0], :]\n",
    "y_train = y_data[cur_index[0], :]\n",
    "\n",
    "# testing data\n",
    "x_test = x_data[cur_index[1], :]\n",
    "y_test = y_data[cur_index[1], :]\n",
    "\n",
    "n_labeled = x_train.shape[0]\n",
    "\n",
    "# Placeholders\n",
    "x_input = tf.placeholder(dtype=tf.float32, shape=[None, input_dim])\n",
    "y_target = tf.placeholder(dtype=tf.float32, shape=[None, 2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parameters for NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "input_dim = 345901\n",
    "n_l1 = 500\n",
    "n_l2 = 500\n",
    "batch_size = 80\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.001\n",
    "beta1 = 0.9\n",
    "z_dim = 'NA'\n",
    "results_path = './Results/Basic_NN_Classifier'\n",
    "n_labels = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tensorflow functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def form_results():\n",
    "    \"\"\"\n",
    "    Forms folders for each run to store the tensorboard files, saved models and the log files.\n",
    "    :return: three string pointing to tensorboard, saved models and log paths respectively.\n",
    "    \"\"\"\n",
    "    folder_name = \"/{0}_{1}_{2}_{3}_{4}_{5}_Basic_NN_Classifier\". \\\n",
    "        format(n_l1, n_l2, learning_rate, batch_size, n_epochs, beta1)\n",
    "    tensorboard_path = results_path + folder_name + '/Tensorboard'\n",
    "    saved_model_path = results_path + folder_name + '/Saved_models/'\n",
    "    log_path = results_path + folder_name + '/log'\n",
    "    if not os.path.exists(results_path + folder_name):\n",
    "        os.mkdir(results_path + folder_name)\n",
    "        os.mkdir(tensorboard_path)\n",
    "        os.mkdir(saved_model_path)\n",
    "        os.mkdir(log_path)\n",
    "    return tensorboard_path, saved_model_path, log_path\n",
    "\n",
    "\n",
    "def next_batch(x, y, batch_size):\n",
    "    \"\"\"\n",
    "    Used to return a random batch from the given inputs.\n",
    "    :param x: Input vector of shape [None, 345901]\n",
    "    :param y: Input labels of shape [None, 2]\n",
    "    :param batch_size: integer, batch size of images and labels to return\n",
    "    :return: x -> [batch_size, 345901], y-> [batch_size, 2]\n",
    "    \"\"\"\n",
    "    index = np.arange(n_labeled)\n",
    "    random_index = np.random.permutation(index)[:batch_size]\n",
    "    return x[random_index], y[random_index]\n",
    "\n",
    "\n",
    "def dense(x, n1, n2, name):\n",
    "    \"\"\"\n",
    "    Used to create a dense layer.\n",
    "    :param x: input tensor to the dense layer\n",
    "    :param n1: no. of input neurons\n",
    "    :param n2: no. of output neurons\n",
    "    :param name: name of the entire dense layer.\n",
    "    :return: tensor with shape [batch_size, n2]\n",
    "    \"\"\"\n",
    "    with tf.name_scope(name):\n",
    "        weights = tf.Variable(tf.random_normal(shape=[n1, n2], mean=0., stddev=0.01), name='weights')\n",
    "        bias = tf.Variable(tf.zeros(shape=[n2]), name='bias')\n",
    "        output = tf.add(tf.matmul(x, weights), bias, name='output')\n",
    "        return output\n",
    "\n",
    "\n",
    "# Dense Network\n",
    "def dense_nn(x):\n",
    "    \"\"\"\n",
    "    Network used to classify MNIST digits.\n",
    "    :param x: tensor with shape [batch_size, 784], input to the dense fully connected layer.\n",
    "    :return: [batch_size, 10], logits of dense fully connected.\n",
    "    \"\"\"\n",
    "    dense_1 = tf.nn.dropout(tf.nn.relu(dense(x, input_dim, n_l1, 'dense_1')), keep_prob=0.25)\n",
    "    dense_2 = tf.nn.dropout(tf.nn.relu(dense(dense_1, n_l1, n_l2, 'dense_2')), keep_prob=0.25)\n",
    "    dense_3 = dense(dense_2, n_l2, n_labels, 'dense_3')\n",
    "    return dense_3\n",
    "\n",
    "\n",
    "def train():\n",
    "    \"\"\"\n",
    "    Used to train the autoencoder by passing in the necessary inputs.\n",
    "    :return: does not return anything\n",
    "    \"\"\"\n",
    "    dense_output = dense_nn(x_input)\n",
    "    # Loss function\n",
    "    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=dense_output, labels=y_target))\n",
    "    # Optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=beta1).minimize(loss)\n",
    "    # Accuracy\n",
    "    pred_op = tf.equal(tf.argmax(dense_output, 1), tf.argmax(y_target, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(pred_op, dtype=tf.float32))\n",
    "    # Summary\n",
    "    tf.summary.scalar(name='Loss', tensor=loss)\n",
    "    tf.summary.scalar(name='Accuracy', tensor=accuracy)\n",
    "    summary_op = tf.summary.merge_all()\n",
    "    saver = tf.train.Saver()\n",
    "    init = tf.global_variables_initializer()\n",
    "    step = 0\n",
    "    with tf.Session() as sess:\n",
    "        tensorboard_path, saved_model_path, log_path = form_results()\n",
    "        #x_l, y_l = mnist.test.next_batch(n_labeled)\n",
    "        writer = tf.summary.FileWriter(logdir=tensorboard_path, graph=sess.graph)\n",
    "        sess.run(init)\n",
    "        for e in range(1, n_epochs + 1):\n",
    "            n_batches = int(n_labeled / batch_size)\n",
    "            print(n_batches)\n",
    "            for b in range(1, n_batches + 1):\n",
    "                batch_x_l, batch_y_l = next_batch(x_train, y_train, batch_size=batch_size)\n",
    "                #print(\"shape:\")\n",
    "                #print(batch_x_l.shape)\n",
    "                sess.run(optimizer, feed_dict={x_input: batch_x_l, y_target: batch_y_l})\n",
    "                #if b % 5 == 0:\n",
    "                #    print(batch_x_l.shape)\n",
    "                loss_, summary = sess.run([loss, summary_op], feed_dict={x_input: batch_x_l, y_target: batch_y_l})\n",
    "                writer.add_summary(summary, step)\n",
    "                print(\"Epoch: {} Iteration: {}\".format(e, b))\n",
    "                print(\"Loss: {}\".format(loss_))\n",
    "                with open(log_path + '/log.txt', 'a') as log:\n",
    "                    log.write(\"Epoch: {}, iteration: {}\\n\".format(e, b))\n",
    "                    log.write(\"Loss: {}\\n\".format(loss_))\n",
    "                step += 1\n",
    "            acc = 0\n",
    "            #num_batches = int(100/ batch_size)\n",
    "            #for j in range(num_batches):\n",
    "            # Classify unseen validation data instead of test data or train data\n",
    "            #batch_x_l, batch_y_l = mnist.validation.next_batch(batch_size=batch_size)\n",
    "            val_acc = sess.run(accuracy, feed_dict={x_input: x_test, y_target: y_test})\n",
    "            #acc += val_acc\n",
    "            #acc /= num_batches\n",
    "            acc = val_acc\n",
    "            print(\"Classification Accuracy: {}\".format(acc))\n",
    "            with open(log_path + '/log.txt', 'a') as log:\n",
    "                log.write(\"Classification Accuracy: {} \\n\".format(acc))\n",
    "            saver.save(sess, save_path=saved_model_path, global_step=step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Epoch: 1 Iteration: 1\n",
      "Loss: 3.396475315093994\n",
      "Classification Accuracy: 0.4000000059604645\n",
      "1\n",
      "Epoch: 2 Iteration: 1\n",
      "Loss: 3.6416561603546143\n",
      "Classification Accuracy: 0.5\n",
      "1\n",
      "Epoch: 3 Iteration: 1\n",
      "Loss: 4.114490032196045\n",
      "Classification Accuracy: 0.44999998807907104\n",
      "1\n",
      "Epoch: 4 Iteration: 1\n",
      "Loss: 2.9879531860351562\n",
      "Classification Accuracy: 0.550000011920929\n",
      "1\n",
      "Epoch: 5 Iteration: 1\n",
      "Loss: 3.0251593589782715\n",
      "Classification Accuracy: 0.6000000238418579\n",
      "1\n",
      "Epoch: 6 Iteration: 1\n",
      "Loss: 4.554908752441406\n",
      "Classification Accuracy: 0.550000011920929\n",
      "1\n",
      "Epoch: 7 Iteration: 1\n",
      "Loss: 3.2393805980682373\n",
      "Classification Accuracy: 0.4000000059604645\n",
      "1\n",
      "Epoch: 8 Iteration: 1\n",
      "Loss: 1.7945884466171265\n",
      "Classification Accuracy: 0.4000000059604645\n",
      "1\n",
      "Epoch: 9 Iteration: 1\n",
      "Loss: 1.6043918132781982\n",
      "Classification Accuracy: 0.6000000238418579\n",
      "1\n",
      "Epoch: 10 Iteration: 1\n",
      "Loss: 1.966596007347107\n",
      "Classification Accuracy: 0.550000011920929\n",
      "1\n",
      "Epoch: 11 Iteration: 1\n",
      "Loss: 1.4396324157714844\n",
      "Classification Accuracy: 0.5\n",
      "1\n",
      "Epoch: 12 Iteration: 1\n",
      "Loss: 1.032598853111267\n",
      "Classification Accuracy: 0.6000000238418579\n",
      "1\n",
      "Epoch: 13 Iteration: 1\n",
      "Loss: 0.9307053685188293\n",
      "Classification Accuracy: 0.3499999940395355\n",
      "1\n",
      "Epoch: 14 Iteration: 1\n",
      "Loss: 0.9454336166381836\n",
      "Classification Accuracy: 0.5\n",
      "1\n",
      "Epoch: 15 Iteration: 1\n",
      "Loss: 0.6846991777420044\n",
      "Classification Accuracy: 0.44999998807907104\n",
      "1\n",
      "Epoch: 16 Iteration: 1\n",
      "Loss: 0.37336841225624084\n",
      "Classification Accuracy: 0.550000011920929\n",
      "1\n",
      "Epoch: 17 Iteration: 1\n",
      "Loss: 0.47554293274879456\n",
      "Classification Accuracy: 0.75\n",
      "1\n",
      "Epoch: 18 Iteration: 1\n",
      "Loss: 0.3777023255825043\n",
      "Classification Accuracy: 0.3499999940395355\n",
      "1\n",
      "Epoch: 19 Iteration: 1\n",
      "Loss: 0.20343764126300812\n",
      "Classification Accuracy: 0.550000011920929\n",
      "1\n",
      "Epoch: 20 Iteration: 1\n",
      "Loss: 0.23454618453979492\n",
      "Classification Accuracy: 0.6499999761581421\n",
      "1\n",
      "Epoch: 21 Iteration: 1\n",
      "Loss: 0.20850686728954315\n",
      "Classification Accuracy: 0.550000011920929\n",
      "1\n",
      "Epoch: 22 Iteration: 1\n",
      "Loss: 0.13416925072669983\n",
      "Classification Accuracy: 0.5\n",
      "1\n",
      "Epoch: 23 Iteration: 1\n",
      "Loss: 0.13292211294174194\n",
      "Classification Accuracy: 0.699999988079071\n",
      "1\n",
      "Epoch: 24 Iteration: 1\n",
      "Loss: 0.08866556733846664\n",
      "Classification Accuracy: 0.44999998807907104\n",
      "1\n",
      "Epoch: 25 Iteration: 1\n",
      "Loss: 0.0840158462524414\n",
      "Classification Accuracy: 0.4000000059604645\n",
      "1\n",
      "Epoch: 26 Iteration: 1\n",
      "Loss: 0.16397646069526672\n",
      "Classification Accuracy: 0.6000000238418579\n",
      "1\n",
      "Epoch: 27 Iteration: 1\n",
      "Loss: 0.09781727939844131\n",
      "Classification Accuracy: 0.6499999761581421\n",
      "1\n",
      "Epoch: 28 Iteration: 1\n",
      "Loss: 0.06366634368896484\n",
      "Classification Accuracy: 0.6000000238418579\n",
      "1\n",
      "Epoch: 29 Iteration: 1\n",
      "Loss: 0.06367279589176178\n",
      "Classification Accuracy: 0.44999998807907104\n",
      "1\n",
      "Epoch: 30 Iteration: 1\n",
      "Loss: 0.09984348714351654\n",
      "Classification Accuracy: 0.5\n",
      "1\n",
      "Epoch: 31 Iteration: 1\n",
      "Loss: 0.06847484409809113\n",
      "Classification Accuracy: 0.6000000238418579\n",
      "1\n",
      "Epoch: 32 Iteration: 1\n",
      "Loss: 0.09757072478532791\n",
      "Classification Accuracy: 0.5\n",
      "1\n",
      "Epoch: 33 Iteration: 1\n",
      "Loss: 0.05704229325056076\n",
      "Classification Accuracy: 0.550000011920929\n",
      "1\n",
      "Epoch: 34 Iteration: 1\n",
      "Loss: 0.06555905193090439\n",
      "Classification Accuracy: 0.4000000059604645\n",
      "1\n",
      "Epoch: 35 Iteration: 1\n",
      "Loss: 0.04799633100628853\n",
      "Classification Accuracy: 0.6000000238418579\n",
      "1\n",
      "Epoch: 36 Iteration: 1\n",
      "Loss: 0.05670217424631119\n",
      "Classification Accuracy: 0.550000011920929\n",
      "1\n",
      "Epoch: 37 Iteration: 1\n",
      "Loss: 0.09966820478439331\n",
      "Classification Accuracy: 0.3499999940395355\n",
      "1\n",
      "Epoch: 38 Iteration: 1\n",
      "Loss: 0.10255434364080429\n",
      "Classification Accuracy: 0.5\n",
      "1\n",
      "Epoch: 39 Iteration: 1\n",
      "Loss: 0.15102079510688782\n",
      "Classification Accuracy: 0.44999998807907104\n",
      "1\n",
      "Epoch: 40 Iteration: 1\n",
      "Loss: 0.1026759147644043\n",
      "Classification Accuracy: 0.4000000059604645\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
